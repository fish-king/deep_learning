边缘检测：
|1 0 -1| 3×3垂直检测滤波
|1 0 -1|
|1 0 -1|

| 1  1  1| 3×3水平检测滤波
| 0  0  0|
|-1 -1 -1|
深度学习将滤波器参数作为学习对象，得到不同特征检测的滤波
|w1 w2 w3 |
|w4 w5 w6 |
|w7 w8 w9 |

卷积层:对图像特征的提取

padding:对图像进行补充，一般n*n图像经m*m滤波器卷积后，图像大小变为n-m+1*n-m+1大小(valid convolution)，若要求图像大小不改变(same convolution)，则对原图像扩充，padding= (f-1)/2

strike:每次卷积的步长

滤波器
f[l] = filtersize
p[l] = padding
s[l] = strike
nc[l] = num of filters
l层滤波器大小为f[l] × f[l] × nc[l-1]

图像
input:nh[l-1]×nw[l-1] × nc[l-1]
output:nh[l]×nw[l] × nc[l]
nw = [(nw[l-1] + 2p[l] - f[l]) / s[l] + 1]向下取整
nh = [(nh[l-1] + 2p[l] - f[l]) / s[l] + 1]向下取整
weights: f[l] × f[l] × nc[l-1] × nc[l]
bias: nc[l]  (1 × 1 × 1 × nc[l])

池化层:不包含学习参数，一般不单独认为一层
filter size:滤波器大小
strike：每次步长

maxpooling:选取滤波器size内最大的一个替代，起到特征强化的左右(常用)
averagepooling：选取滤波器size内均值一个替代，在较深层次网络中使用

全连接层:卷积层结果拉伸成一维,进行标准神经网络训练
model.add(layers.Flatten()) 一步到位！

卷积神经网络的参数少于全连接网络的原因：
参数共享:一个卷积核能应用于输入图像的多个位置
稀疏连接:输出层的一个像素仅与原图上卷积核大小区间相关

经典网络
LeNet-5:
32×32×1--conv,5×5,s=1-->28×28×6--avg pool,f=2,s=2-->14×14×6--conv,5×5,s=1-->10×10×16--avg pool,f=2,s=2-->5×5×16--FC-->120--FC-->84---->y~

AlexNet:
227×227×3--conv,11×11,s=4-->55×55×96--max pool,f=3,s=2-->27×27×96--conv,5×5,same-->27×27×256--max pool,f=3,s=2-->13×13×256--conv,3×3,same-->
13×13×384--conv,3×3,same-->13×13×384--conv,3×3,same-->13×13×256--max pool,f=3,s=2-->6×6×256--FC-->9216--FC-->4096--FC-->4096--FC,softmax-->1000

VGG-16:
0 is conv1_1 (3, 3, 3, 64)
1 is relu activation function 
2 is conv1_2 (3, 3, 64, 64)
3 is relu    
4 is maxpool
5 is conv2_1 (3, 3, 64, 128)
6 is relu
7 is conv2_2 (3, 3, 128, 128)
8 is relu
9 is maxpool
10 is conv3_1 (3, 3, 128, 256)
11 is relu
12 is conv3_2 (3, 3, 256, 256)
13 is relu
14 is conv3_3 (3, 3, 256, 256)
15 is relu
16 is conv3_4 (3, 3, 256, 256)
17 is relu
18 is maxpool
19 is conv4_1 (3, 3, 256, 512)
20 is relu
21 is conv4_2 (3, 3, 512, 512)
22 is relu
23 is conv4_3 (3, 3, 512, 512)
24 is relu
25 is conv4_4 (3, 3, 512, 512)
26 is relu
27 is maxpool
28 is conv5_1 (3, 3, 512, 512)
29 is relu
30 is conv5_2 (3, 3, 512, 512)
31 is relu
32 is conv5_3 (3, 3, 512, 512)
33 is relu
34 is conv5_4 (3, 3, 512, 512)
35 is relu
36 is maxpool
37 is fullyconnected (7, 7, 512, 4096)
38 is relu
39 is fullyconnected (1, 1, 4096, 4096)
40 is relu
41 is fullyconnected (1, 1, 4096, 1000)
42 is softmax

残差网络(ResNetwork):将某一层的未激活前输出z[i]加到后面某层未激活前输出z[i+n] = z[i+n]+z[i]，缺省用0填补
加大网络前后关联性，减小梯度爆炸或梯度消失情况发生

1×1卷积:可以理解为对通道维度的卷积，可以增加或减少通道的数量，在某两卷积间作为中介，可减少运算量

Inception 网络:每一层将输入用不通卷积核池化层卷积池化后结果拼接，作为特殊的卷积层，让机器自己识别那个效果好，机器牛逼就是为所欲为的

迁移学习:相同网络结构可以下载别人训练好的参数，作为初始化进行训练，也可以将前几层参数冻结，只训练输出层内容，节约训练时间

数据扩充:对数据进行裁剪，镜像，color shifting等操作扩展数据库，优秀的数据集对网络准确率影响最大

滑动窗口检测:通过一个在图像上滑行的窗口，每次对图像上是否有目标进行分类检测，输出结果即为窗口框
滑动窗口检测可通过卷积实现，若分为n×n窗口，则输出层为n×n×size，区别于非卷积1×1×size









